{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40658427-be3a-49c1-99bb-98aadb67b572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turbines found: ['T1', 'T2', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15']\n",
      "\n",
      "T1 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 4\n",
      "After NaN filters: 4\n",
      "T1_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 4.57% / 4.63%\n",
      "T1_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 4.73% / 4.82%\n",
      "T1_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.11% / 3.17%\n",
      "T1_DS21 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.29% / 3.73%\n",
      "\n",
      "T2 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 2\n",
      "After NaN filters: 2\n",
      "T2_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 2.54% / 2.60%\n",
      "T2_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 4.82% / 4.91%\n",
      "\n",
      "T4 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 0\n",
      "After NaN filters: 0\n",
      "\n",
      "T5 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 3\n",
      "After NaN filters: 3\n",
      "T5_DS7 → Features: 362 → Clean: 116 → NaNs (target/vars): 2.40% / 2.28%\n",
      "T5_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.72% / 5.78%\n",
      "T5_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.43% / 5.51%\n",
      "\n",
      "T6 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 0\n",
      "After NaN filters: 0\n",
      "\n",
      "T7 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 2\n",
      "After NaN filters: 2\n",
      "T7_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 4.61% / 4.68%\n",
      "T7_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.00% / 3.09%\n",
      "\n",
      "T8 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 4\n",
      "After NaN filters: 4\n",
      "T8_DS16 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.51% / 5.64%\n",
      "T8_DS17 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.88% / 5.98%\n",
      "T8_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 7.31% / 7.37%\n",
      "T8_DS19 → Features: 362 → Clean: 121 → NaNs (target/vars): 7.67% / 7.69%\n",
      "\n",
      "T9 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 3\n",
      "After NaN filters: 3\n",
      "T9_DS7 → Features: 362 → Clean: 116 → NaNs (target/vars): 2.75% / 2.71%\n",
      "T9_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 1.77% / 1.84%\n",
      "T9_DS21 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.99% / 4.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daragh\\AppData\\Local\\Temp\\ipykernel_25888\\2059745538.py:76: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  alarms['Timestamp end'] = pd.to_datetime(alarms['Timestamp end'], errors='coerce').dt.ceil('10min')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T10 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 5\n",
      "After NaN filters: 5\n",
      "T10_DS7 → Features: 362 → Clean: 116 → NaNs (target/vars): 5.92% / 5.88%\n",
      "T10_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 2.22% / 2.28%\n",
      "T10_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 1.97% / 2.06%\n",
      "T10_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 2.14% / 2.19%\n",
      "T10_DS21 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.86% / 4.31%\n",
      "\n",
      "T11 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 3\n",
      "After NaN filters: 3\n",
      "T11_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.60% / 5.67%\n",
      "T11_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.51% / 5.60%\n",
      "T11_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 5.64% / 5.71%\n",
      "\n",
      "T12 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 1\n",
      "After NaN filters: 1\n",
      "T12_DS7 → Features: 362 → Clean: 116 → NaNs (target/vars): 3.39% / 3.37%\n",
      "\n",
      "T13 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 2\n",
      "After NaN filters: 2\n",
      "T13_DS7 → Features: 362 → Clean: 116 → NaNs (target/vars): 2.73% / 2.73%\n",
      "T13_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 1.92% / 1.94%\n",
      "\n",
      "T14 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 2\n",
      "After NaN filters: 2\n",
      "T14_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.84% / 3.90%\n",
      "T14_DS21 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.37% / 3.82%\n",
      "\n",
      "T15 Summary:\n",
      "Original datasets: 21\n",
      "After critical alarm filter: 3\n",
      "After NaN filters: 3\n",
      "T15_DS18 → Features: 362 → Clean: 123 → NaNs (target/vars): 4.20% / 4.26%\n",
      "T15_DS19 → Features: 362 → Clean: 123 → NaNs (target/vars): 3.14% / 3.23%\n",
      "T15_DS20 → Features: 362 → Clean: 123 → NaNs (target/vars): 2.12% / 2.17%\n",
      "\n",
      "Total datasets across all turbines: 34\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "farm = 'Penmanshiel'  # 'Kelmarsh' or 'Penmanshiel'\n",
    "target_feature = 'Generator bearing rear temperature (°C)'\n",
    "\n",
    "df_start_date = datetime(2019, 1, 1)\n",
    "df_end_date = datetime(2023, 1, 1)\n",
    "\n",
    "train_months = 4\n",
    "valid_months = 2\n",
    "test_months = 1\n",
    "jump_months = 2\n",
    "\n",
    "target_max_nans = 0.1\n",
    "input_max_nans = 0.1\n",
    "\n",
    "# Output directory: where cleaned files will be saved\n",
    "save_dir = './1_healthy_datasets'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ------------------- LOAD DATA -------------------\n",
    "with open(f'./0_raw_farm_dicts/{farm}_ALARMS.pkl', 'rb') as f:\n",
    "    alarm_dict = pickle.load(f)\n",
    "with open(f'./0_raw_farm_dicts/{farm}_SCADA.pkl', 'rb') as f:\n",
    "    scada_dict = pickle.load(f)\n",
    "\n",
    "turbines = list(scada_dict.keys())\n",
    "print(f\"Turbines found: {turbines}\")\n",
    "\n",
    "# ------------------- HELPER FUNCTIONS -------------------\n",
    "def row_contains_strings(row, search_strings):\n",
    "    search_strings = [s.lower() for s in search_strings]\n",
    "    for value in row:\n",
    "        if pd.notna(value) and any(s in str(value).lower() for s in search_strings):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_alarm_in_window(dataset_time_index, critical_alarms):\n",
    "    extended_start = dataset_time_index.min() - timedelta(days=30)\n",
    "    window_end = dataset_time_index.max()\n",
    "    for alarm_time in critical_alarms.index:\n",
    "        if extended_start <= alarm_time <= window_end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def set_downtime_to_nan(df_, stop_alarms, restart_alarms):\n",
    "    df = df_.copy()\n",
    "    stop_alarms = stop_alarms.sort_index()\n",
    "    restart_alarms = restart_alarms.sort_index()\n",
    "    nan_count = 0\n",
    "    for stop_time in stop_alarms.index:\n",
    "        restart_time = restart_alarms.index[restart_alarms.index > stop_time].min()\n",
    "        if pd.notna(restart_time):\n",
    "            df.loc[stop_time:restart_time] = np.nan\n",
    "            nan_count += df.loc[stop_time:restart_time].shape[0]\n",
    "    return df, nan_count\n",
    "\n",
    "# ------------------- PROCESSING -------------------\n",
    "total_datasets = 0\n",
    "combined_dict = {}\n",
    "\n",
    "for turbine in turbines:\n",
    "    # ------------------- 1. Load SCADA and Alarm Data -------------------\n",
    "    df = scada_dict[turbine].loc[df_start_date:df_end_date].copy()\n",
    "    alarms = alarm_dict[turbine].copy()\n",
    "    \n",
    "    alarms['Timestamp start'] = pd.to_datetime(alarms['Timestamp start']).dt.floor('10min')\n",
    "    alarms['Timestamp end'] = pd.to_datetime(alarms['Timestamp end'], errors='coerce').dt.ceil('10min')\n",
    "    alarms = alarms.set_index('Timestamp start')\n",
    "    alarms = alarms[(alarms.index >= df_start_date) & (alarms.index <= df_end_date)]\n",
    "\n",
    "    # ------------------- 2. Keep only mean-value features -------------------\n",
    "    exclude_terms = ['Max', 'Min', 'Standard deviation', 'StdDev', 'std', 'counter']\n",
    "    filtered_df = df[[col for col in df.columns if not any(term.lower() in col.lower() for term in exclude_terms)]]\n",
    "\n",
    "    # ------------------- 3. Define Dataset Time Windows -------------------\n",
    "    start_end_dates = {}\n",
    "    start_pointer = df_start_date\n",
    "    ds_number = 1\n",
    "\n",
    "    while (start_pointer + pd.DateOffset(months=train_months + valid_months + test_months)) < df_end_date:\n",
    "        end_pointer = start_pointer + pd.DateOffset(months=train_months + valid_months + test_months)\n",
    "        start_end_dates[f\"{turbine}_DS{ds_number}\"] = [start_pointer, end_pointer]\n",
    "        start_pointer += pd.DateOffset(months=jump_months)\n",
    "        ds_number += 1\n",
    "\n",
    "    # ------------------- 4. Create Datasets -------------------\n",
    "    datasets = {\n",
    "        name: filtered_df[dates[0]:dates[1]].iloc[:-1]\n",
    "        for name, dates in start_end_dates.items()\n",
    "    }\n",
    "\n",
    "    # ------------------- 5. Remove Datasets with Critical Failures -------------------\n",
    "    crit_strings = ['generator fan', 'nde']\n",
    "    crit_alarms = alarms[alarms.apply(lambda row: row_contains_strings(row, crit_strings), axis=1)]\n",
    "\n",
    "    datasets = {\n",
    "        name: data for name, data in datasets.items()\n",
    "        if not check_alarm_in_window(data.index, crit_alarms)\n",
    "    }\n",
    "\n",
    "    # ------------------- 6. Remove Maintenance Periods -------------------\n",
    "    stop_strings = ['Forced outage', 'Scheduled Maintenance', 'Requested Shutdown', 'icing']\n",
    "    restart_strings = ['Mains operation']\n",
    "\n",
    "    stop_alarms = alarms[alarms.apply(lambda row: row_contains_strings(row, stop_strings), axis=1)]\n",
    "    restart_alarms = alarms[alarms.apply(lambda row: row_contains_strings(row, restart_strings), axis=1)]\n",
    "\n",
    "    healthy_datasets = {}\n",
    "\n",
    "    for name, data in datasets.items():\n",
    "        data, nan_count = set_downtime_to_nan(data, stop_alarms, restart_alarms)\n",
    "\n",
    "        # ------------------- 7. Remove High-Nan Target Datasets -------------------\n",
    "        if data[target_feature].isna().mean() > target_max_nans:\n",
    "            continue\n",
    "\n",
    "        # ------------------- 8. Remove Columns with Too Many NaNs -------------------\n",
    "        nan_frac = data.isna().mean()\n",
    "        data = data.loc[:, nan_frac <= input_max_nans]\n",
    "        healthy_datasets[name] = data\n",
    "\n",
    "    # ------------------- 9. Print Summary -------------------\n",
    "    print(f\"\\n{turbine} Summary:\")\n",
    "    print(f\"Original datasets: {len(start_end_dates)}\")\n",
    "    print(f\"After critical alarm filter: {len(datasets)}\")\n",
    "    print(f\"After NaN filters: {len(healthy_datasets)}\")\n",
    "\n",
    "    for name, ds in healthy_datasets.items():\n",
    "        tgt_nan = ds[target_feature].isna().mean() * 100\n",
    "        var_nan = ds.drop(columns=[target_feature]).isna().mean().mean() * 100\n",
    "        print(f\"{name} → Features: {df.shape[1]} → Clean: {len(ds.columns)} → NaNs (target/vars): {tgt_nan:.2f}% / {var_nan:.2f}%\")\n",
    "\n",
    "    # ------------------- 10. Split into Train/Val/Test -------------------\n",
    "    dataset_dict = {}\n",
    "    for name, ds in healthy_datasets.items():\n",
    "        start, end = start_end_dates[name]\n",
    "        dataset_dict[name] = {\n",
    "            'train': ds[start:start + pd.DateOffset(months=train_months)].iloc[:-1],\n",
    "            'valid': ds[start + pd.DateOffset(months=train_months):\n",
    "                        start + pd.DateOffset(months=train_months + valid_months)].iloc[:-1],\n",
    "            'test':  ds[start + pd.DateOffset(months=train_months + valid_months): end]\n",
    "        }\n",
    "\n",
    "    # ------------------- 11. Add to Combined Dictionary -------------------\n",
    "    for name, split in dataset_dict.items():\n",
    "        tid = name.split('_')[0]\n",
    "        ds_id = name.split('_')[1]\n",
    "        combined_dict[f\"{tid}_{ds_id}\"] = split\n",
    "\n",
    "    total_datasets += len(dataset_dict)\n",
    "\n",
    "# ------------------- 12. Save Combined Output -------------------\n",
    "output_path = f'{save_dir}/{farm}_HealthyDatasets.pkl'\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(combined_dict, f)\n",
    "\n",
    "print(f\"\\nTotal datasets across all turbines: {total_datasets}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
