{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969f743-819a-4ca2-8b64-6aed7fe27978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTIONAL TOOL: Unzip files from Zenodo\"\"\"\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_scada_archives(raw_dir, farm):\n",
    "    \"\"\"\n",
    "    Unzips all .zip files in the given raw_dir to a subfolder `unzipped_raw/{folder_name}`\n",
    "    \"\"\"\n",
    "    unzip_dir = os.path.join(raw_dir, 'unzipped_raw')\n",
    "    os.makedirs(unzip_dir, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(raw_dir):\n",
    "        if file.endswith('.zip'):\n",
    "            zip_path = os.path.join(raw_dir, file)\n",
    "            folder_name = os.path.splitext(file)[0]\n",
    "            extract_path = os.path.join(unzip_dir, folder_name)\n",
    "\n",
    "            if not os.path.exists(extract_path):\n",
    "                print(f\"Unzipping {file} to {extract_path}...\")\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_path)\n",
    "            else:\n",
    "                print(f\"Already unzipped: {file}\")\n",
    "\n",
    "# Example usage\n",
    "farm = 'Penmanshiel'  # or 'Penmanshiel'\n",
    "raw_dir = f'./{farm}_raw_folder'\n",
    "unzip_scada_archives(raw_dir, farm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed2338c-b54a-4c85-aa43-acdffae9b648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:11<00:00, 18.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:18<00:00, 19.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:19<00:00, 19.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:19<00:00, 19.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:20<00:00, 20.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:21<00:00, 20.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:18<00:00, 19.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:17<00:00, 19.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:13<00:00, 19.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:12<00:00, 18.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:12<00:00, 18.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:12<00:00, 18.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:15<00:00, 19.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on T15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:14<00:00, 19.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved farm dictionary to Penmanshiel_SCADA.pkl\n",
      "Working on T1...\n",
      "→ Added 58087 rows for T1\n",
      "Working on T2...\n",
      "→ Added 66975 rows for T2\n",
      "Working on T4...\n",
      "→ Added 56727 rows for T4\n",
      "Working on T5...\n",
      "→ Added 71650 rows for T5\n",
      "Working on T6...\n",
      "→ Added 62319 rows for T6\n",
      "Working on T7...\n",
      "→ Added 71707 rows for T7\n",
      "Working on T8...\n",
      "→ Added 61006 rows for T8\n",
      "Working on T9...\n",
      "→ Added 65862 rows for T9\n",
      "Working on T10...\n",
      "→ Added 69071 rows for T10\n",
      "Working on T11...\n",
      "→ Added 52611 rows for T11\n",
      "Working on T12...\n",
      "→ Added 46535 rows for T12\n",
      "Working on T13...\n",
      "→ Added 52376 rows for T13\n",
      "Working on T14...\n",
      "→ Added 55961 rows for T14\n",
      "Working on T15...\n",
      "→ Added 48416 rows for T15\n",
      "Saved alarm dictionary to ./0_raw_farm_dicts\\Penmanshiel_ALARMS.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "# Define the time range of interest\n",
    "yr_start, yr_end = 2016, 2022\n",
    "\n",
    "# Set the farm to process\n",
    "farm = 'Penmanshiel'  # Change to 'Kelmarsh' or 'Penmanshiel' as needed\n",
    "\n",
    "# List of turbine IDs for each farm\n",
    "turbines = {\n",
    "    'Kelmarsh': [1, 2, 3, 4, 5, 6],\n",
    "    'Penmanshiel': [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "}\n",
    "\n",
    "# Generate the expected full date range (10-min frequency)\n",
    "start_date = datetime(yr_start, 1, 1)\n",
    "end_date = datetime(yr_end + 1, 1, 1)\n",
    "date_list = pd.date_range(start_date, end_date, freq=\"10min\")[:-1]\n",
    "\n",
    "# Input directory: where unzipped SCADA data lives. NOTE: you must set this up correctly, either use above tool or assign directory to your folder.\n",
    "raw_dir = f'./{farm}_raw_folder'\n",
    "\n",
    "# Output directory: where cleaned files will be saved\n",
    "save_dir = './0_raw_farm_dicts'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------------------- CLEANING FUNCTION -------------------\n",
    "def CleanDeleteAndPad_ScadaData(df):\n",
    "    \"\"\"\n",
    "    Cleans and aligns SCADA data to 10-minute intervals:\n",
    "    - Converts time strings to datetime\n",
    "    - Removes invalid timestamps\n",
    "    - Pads missing timestamps with NaNs\n",
    "    \"\"\"\n",
    "    time_col = df.columns[0]\n",
    "\n",
    "    # Convert timestamp column to datetime objects\n",
    "    df[time_col] = pd.to_datetime(df[time_col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # df[time_col] = pd.to_datetime(df[time_col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    df = df.dropna(subset=[time_col]).sort_values(by=time_col).reset_index(drop=True)\n",
    "\n",
    "    # Initialize fixed row list with first valid row\n",
    "    fixed_rows = [df.loc[0]]\n",
    "    prev_time = df.loc[0, time_col]\n",
    "\n",
    "    # Loop through the rest of the DataFrame\n",
    "    for i in range(1, len(df)):\n",
    "        current_time = df.loc[i, time_col]\n",
    "\n",
    "        # Insert NaN rows if time gap > 10 minutes\n",
    "        while current_time - prev_time > timedelta(minutes=10):\n",
    "            missing_time = prev_time + timedelta(minutes=10)\n",
    "            new_row = df.loc[i - 1].copy()\n",
    "            new_row[:] = np.nan\n",
    "            new_row[time_col] = missing_time\n",
    "            fixed_rows.append(new_row)\n",
    "            prev_time = missing_time\n",
    "\n",
    "        # Skip early/duplicate readings\n",
    "        if current_time - prev_time < timedelta(minutes=10):\n",
    "            continue\n",
    "\n",
    "        fixed_rows.append(df.loc[i])\n",
    "        prev_time = current_time\n",
    "\n",
    "    # Return cleaned DataFrame with datetime index\n",
    "    df_clean = pd.DataFrame(fixed_rows).set_index(time_col)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# ------------------- FILE COLLECTION -------------------\n",
    "def find_all_scada_files(farm, raw_dir):\n",
    "    \"\"\"\n",
    "    Walks through all unzipped folders in the raw directory,\n",
    "    and collects all file paths inside SCADA folders.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for folder in os.listdir(raw_dir):\n",
    "        if not folder.startswith(f\"{farm}_SCADA\"):\n",
    "            continue\n",
    "\n",
    "        subdir = os.path.join(raw_dir, folder)\n",
    "\n",
    "        if not os.path.isdir(subdir):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(subdir):\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            files.append(full_path)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "# ------------------- MAIN PROCESS -------------------\n",
    "def process_farm_scada(farm):\n",
    "    \"\"\"\n",
    "    Main SCADA cleaning pipeline:\n",
    "    - Finds all files for selected farm\n",
    "    - Groups data by turbine\n",
    "    - Cleans and pads timestamps\n",
    "    - Saves both individual turbine files and a full-farm pickle\n",
    "    \"\"\"\n",
    "    all_files = find_all_scada_files(farm, raw_dir)\n",
    "    scada_dict = {}\n",
    "\n",
    "    \n",
    "    for t in turbines[farm]:\n",
    "        \n",
    "        t_label = f'T{t}'\n",
    "        print(f'Working on {t_label}:')\n",
    "            \n",
    "        # handle zero padding in PM files\n",
    "        if farm == 'Penmanshiel':\n",
    "            t = str(t).zfill(2)\n",
    "        t_files = [f for f in all_files if f'Turbine_Data_{farm}_{t}_' in f]\n",
    "        t_df = pd.DataFrame()\n",
    "\n",
    "        for f in tqdm(t_files):\n",
    "            try:\n",
    "                df = pd.read_csv(f, header=9)  # SCADA data starts at line 10\n",
    "                df = CleanDeleteAndPad_ScadaData(df)\n",
    "                df = df.drop(df.columns[0], axis=1)  # Drop original time col (we now use index)\n",
    "                \n",
    "                df.index = pd.to_datetime(df.index, errors='coerce') # set the index \n",
    "                df.index.name = 'Timestamp'\n",
    "                df = df[~df.index.isna()]  # drop any broken timestamps\n",
    "\n",
    "                t_df = pd.concat([t_df, df])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {f}: {e}\")\n",
    "\n",
    "        if not t_df.empty:\n",
    "            t_df.index = pd.to_datetime(t_df.index)\n",
    "\n",
    "            # If turbine data starts after desired start_date, prepend NaNs\n",
    "            if t_df.index[0] > start_date:\n",
    "                missing_rows = len(date_list) - len(t_df)\n",
    "                prepend = pd.DataFrame(np.nan, index=date_list[:missing_rows], columns=t_df.columns)\n",
    "                t_df = pd.concat([prepend, t_df])\n",
    "            \n",
    "            #store in dict\n",
    "            scada_dict[t_label] = t_df\n",
    "            \n",
    "        else:\n",
    "            print(f\"No data found for turbine {t}\")\n",
    "\n",
    "    # Save the entire farm dataset as a dictionary\n",
    "    with open(os.path.join(save_dir, f'{farm}_SCADA.pkl'), 'wb') as f:\n",
    "        pickle.dump(scada_dict, f)\n",
    "\n",
    "    print(f\"Saved farm dictionary to {farm}_SCADA.pkl\")\n",
    "    \n",
    "def process_farm_alarms(farm):\n",
    "    \"\"\"\n",
    "    Builds a dictionary of turbine alarms:\n",
    "    - For each turbine, finds all Status_* CSVs\n",
    "    - Concatenates them into a single DataFrame\n",
    "    - Saves result as {Tn: DataFrame} in alarm_dict\n",
    "    \"\"\"\n",
    "    all_files = find_all_scada_files(farm, raw_dir)\n",
    "    alarm_dict = {}\n",
    "\n",
    "    for t in turbines[farm]:\n",
    "        t_label = f'T{int(t)}'  # Ensure no zero-padding in dict key\n",
    "        print(f'Working on {t_label}...')\n",
    "\n",
    "        if farm == 'Penmanshiel': # Match zero-padded turbine numbers in filenames for Penmanshiel\n",
    "            t = str(t).zfill(2)\n",
    "        t_files = [f for f in all_files if f'Status_{farm}_{t}_' in f]\n",
    "\n",
    "        t_df = pd.DataFrame()\n",
    "\n",
    "        for f in t_files:\n",
    "            try:\n",
    "                df = pd.read_csv(f, header=9)  # Same structure as SCADA files\n",
    "                t_df = pd.concat([t_df, df])\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {f}: {e}\")\n",
    "        \n",
    "        if not t_df.empty:\n",
    "            t_df = t_df.reset_index()\n",
    "            alarm_dict[t_label] = t_df\n",
    "            print(f\"→ Added {len(t_df)} rows for {t_label}\")\n",
    "        else:\n",
    "            print(f\"→ No data found for {t_label}\")\n",
    "\n",
    "    # Save dictionary\n",
    "    output_path = os.path.join(save_dir, f'{farm}_ALARMS.pkl')\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(alarm_dict, f)\n",
    "\n",
    "    print(f\"Saved alarm dictionary to {output_path}\")\n",
    "\n",
    "# ------------------- EXECUTE -------------------\n",
    "process_farm_scada(farm)\n",
    "process_farm_alarms(farm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
