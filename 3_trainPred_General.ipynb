{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f563612e-d6f7-4f96-b733-902478f2ef43",
   "metadata": {},
   "source": [
    "This code will train one model at a time on however many datasets you wish.\n",
    "HP are randomly selected from a grid of appropriate hp's for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0968a3e-ccff-4d88-809e-8b04f32be86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for Farm: Kelmarsh, Model: cnn, sim: 1\n",
      "\n",
      "-- FS Method: LC --\n",
      "Model has 4,898 trainable parameters\n",
      "Epoch 2 took 2.1722 seconds\n",
      "Epoch 2/200: best validation loss so far: 13.1466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 149\u001b[0m\n\u001b[0;32m    147\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    148\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 149\u001b[0m best_model_state, best_valid_loss \u001b[38;5;241m=\u001b[39m train_valid_model_fast(model, criterion, optimizer, train_loader, valid_loader, hp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m    152\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - Trinity College Dublin\\Documents\\Anomaly_Benchmark\\InputSelection_Github\\Utils\\prediction_utils.py:137\u001b[0m, in \u001b[0;36mtrain_valid_model_fast\u001b[1;34m(model, criterion, optimizer, train_loader, valid_loader, num_epochs, clipping)\u001b[0m\n\u001b[0;32m    134\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# === Training === #\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    138\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice), y_batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Skip NaN-containing batches\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\.conda\\envs\\Normal_Custom\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### -------- Part 0: CONFIGURATION -------- ###\n",
    "import os, sys, pickle, random, time, itertools\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Utils.utils import segment_timeseries_v2, TimeseriesDataset, MinMaxScale_datasets\n",
    "from Utils.prediction_models import TemporalCNN_Pr, SimpleFFNN_Pr, SimpleLSTM_Pr\n",
    "from Utils.prediction_utils import train_valid_model_fast, test_trained_model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# --- User-configurable options\n",
    "farm = 'Kelmarsh'  # Can be changed\n",
    "model_choice = 'cnn'  # 'cnn', 'lstm', or 'ffnn'\n",
    "random_seed = 42\n",
    "random_select = 22\n",
    "target_feature = 'Generator bearing rear temperature (Â°C)'\n",
    "sim = 1  # Simulation run number\n",
    "\n",
    "# --- Paths and loading\n",
    "with open(f'./1_healthy_datasets/{farm}_HealthyDatasets.pkl', 'rb') as f:\n",
    "    healthy_datasets = pickle.load(f)\n",
    "with open(f'./2_ordered_inputs/{farm}_ordered_inputs.pkl', 'rb') as f:\n",
    "    chosen_features_dict = pickle.load(f)\n",
    "\n",
    "# --- Normalization\n",
    "healthy_datasets = MinMaxScale_datasets(healthy_datasets, target_feature)\n",
    "\n",
    "# --- Configuration lists\n",
    "dataset_names = [list(healthy_datasets.keys())[1]]  # Or more\n",
    "fs_methods = ['LC', 'DT', 'MI', 'MLPSHAP']\n",
    "pred_models = {\n",
    "    'cnn': TemporalCNN_Pr,\n",
    "    'lstm': SimpleLSTM_Pr,\n",
    "    'ffnn': SimpleFFNN_Pr\n",
    "}\n",
    "\n",
    "### -------- Part 1: HYPERPARAMETER GRID -------- ###\n",
    "fixed_hyperparams = {'steps_ahead': 0}\n",
    "\n",
    "hp_grid = {\n",
    "    'cnn': {\n",
    "        'num_features': [10, 20, 30],\n",
    "        'window_size': [9, 18],\n",
    "        'batch_size': [32, 64],\n",
    "        'num_epochs': [200],\n",
    "        'num_channels': [32, 64],\n",
    "        'num_layers': [1, 2],\n",
    "        'kernel_size': [3, 5],\n",
    "        'learning_rate': [0.001, 0.0001]\n",
    "    },\n",
    "    'ffnn': {\n",
    "        'num_features': [10, 20, 30],\n",
    "        'window_size': [9, 18],\n",
    "        'batch_size': [32, 64],\n",
    "        'num_epochs': [200],\n",
    "        'hidden_size': [64, 128, 192],\n",
    "        'num_layers': [1, 2, 3],\n",
    "        'learning_rate': [0.001, 0.0001]\n",
    "    },\n",
    "    'lstm': {\n",
    "        'num_features': [10, 20, 30],\n",
    "        'window_size': [9, 18],\n",
    "        'batch_size': [32, 64],\n",
    "        'num_epochs': [200],\n",
    "        'hidden_size': [32, 64, 128],\n",
    "        'num_layers_lstm': [1, 2, 3],\n",
    "        'learning_rate': [0.001, 0.0001]\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Set Random seed for HP combinations\n",
    "random.seed(random_seed)\n",
    "param_names = list(hp_grid[model_choice].keys())\n",
    "hp_combinations = list(itertools.product(*hp_grid[model_choice].values()))\n",
    "random_combinations = random.sample(hp_combinations, random_select)\n",
    "\n",
    "# save hp's\n",
    "results_dir = f'./3_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "df_random_combinations = pd.DataFrame(random_combinations, columns=param_names)\n",
    "df_random_combinations.to_csv(f'{results_dir}/HP_combinations_sim{sim}_{model_choice}.csv', index=False)\n",
    "\n",
    "### -------- Part 2: TRAINING LOOP -------- ###\n",
    "trained_models = {}\n",
    "metrics = {}\n",
    "selected_datasets = {}\n",
    "\n",
    "print(f'Running training for Farm: {farm}, Model: {model_choice}, sim: {sim}')\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    train_set = healthy_datasets[dataset_name]['train']\n",
    "    valid_set = healthy_datasets[dataset_name]['valid']\n",
    "    test_set = healthy_datasets[dataset_name]['test']\n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        print(f'\\n-- FS Method: {fs_method} --')\n",
    "        hp_set_counter = 0\n",
    "        best_loss_for_HP_set = float('inf')\n",
    "\n",
    "        all_features = chosen_features_dict[f'{dataset_name}_{fs_method}']\n",
    "\n",
    "        for combination in random_combinations:\n",
    "            \n",
    "            #zip hps\n",
    "            hp_dict = dict(zip(param_names, combination))\n",
    "\n",
    "            # Data preparation\n",
    "            chosen_features = all_features[:hp_dict['num_features']]\n",
    "            X_train, y_train = segment_timeseries_v2(train_set[chosen_features].values, train_set[target_feature].values, hp_dict['window_size'], fixed_hyperparams['steps_ahead'])\n",
    "            X_valid, y_valid = segment_timeseries_v2(valid_set[chosen_features].values, valid_set[target_feature].values, hp_dict['window_size'], fixed_hyperparams['steps_ahead'])\n",
    "            X_test, y_test = segment_timeseries_v2(test_set[chosen_features].values, test_set[target_feature].values, hp_dict['window_size'], fixed_hyperparams['steps_ahead'])\n",
    "\n",
    "            train_loader = DataLoader(TimeseriesDataset(X_train, y_train), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "            valid_loader = DataLoader(TimeseriesDataset(X_valid, y_valid), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "            test_loader = DataLoader(TimeseriesDataset(X_test, y_test), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "\n",
    "            # Model definition\n",
    "            if model_choice == 'cnn':\n",
    "                                    model = pred_models[model_choice](\n",
    "                                        input_size=hp_dict['num_features'],\n",
    "                                        num_channels=hp_dict['num_channels'],\n",
    "                                        num_layers=hp_dict['num_layers'],\n",
    "                                        kernel_size=hp_dict['kernel_size']\n",
    "                                    )\n",
    "\n",
    "            elif model_choice == 'lstm':\n",
    "                                    model = pred_models[model_choice](\n",
    "                                        input_size=hp_dict['num_features'],\n",
    "                                        hidden_size=hp_dict['hidden_size'],\n",
    "                                        num_layers_lstm=hp_dict['num_layers_lstm']\n",
    "                                    )\n",
    "\n",
    "            elif model_choice == 'ffnn':\n",
    "                                    model = pred_models[model_choice](\n",
    "                                        input_size=hp_dict['num_features'] * hp_dict['window_size'],\n",
    "                                        hidden_size=hp_dict['hidden_size'],\n",
    "                                        num_layers=hp_dict['num_layers']\n",
    "                                    )\n",
    "            # Training\n",
    "            print(f'Model has {count_parameters(model):,} trainable parameters')\n",
    "            criterion = nn.MSELoss(reduction='sum')\n",
    "            optimizer = optim.Adam(model.parameters(), lr=hp_dict['learning_rate'])\n",
    "            best_model_state, best_valid_loss = train_valid_model_fast(model, criterion, optimizer, train_loader, valid_loader, hp_dict['num_epochs'])\n",
    "\n",
    "            # Save\n",
    "            key = f'{dataset_name}_{model_choice}_{fs_method}_sim{sim}'\n",
    "            trained_models[key] = best_model_state\n",
    "\n",
    "            # Test\n",
    "            model.load_state_dict(best_model_state)\n",
    "            metrics[key] = {\n",
    "                'test': test_trained_model(model, test_loader),\n",
    "                'valid': test_trained_model(model, valid_loader)\n",
    "            }\n",
    "\n",
    "            # Save artifacts\n",
    "            model_dir = f'./3_results/models/sim{sim}_{model_choice}/{dataset_name}'\n",
    "            metrics_dir = f'./3_results/metrics/sim{sim}_{model_choice}/{dataset_name}'\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "            with open(f'{model_dir}/Model_{farm}_{dataset_name}_{model_choice}_{fs_method}_hpset{hp_set_counter}.pkl', 'wb') as f:\n",
    "                pickle.dump(best_model_state, f)\n",
    "\n",
    "            with open(f'{metrics_dir}/Metrics_{farm}_{dataset_name}_{model_choice}_{fs_method}_hpset{hp_set_counter}.pkl', 'wb') as f:\n",
    "                pickle.dump(metrics[key], f)\n",
    "\n",
    "            print(f'[Set {hp_set_counter+1}/{random_select}] Loss: {best_valid_loss:.4f}')\n",
    "            hp_set_counter += 1\n",
    "\n",
    "            if best_valid_loss < best_loss_for_HP_set:\n",
    "                best_loss_for_HP_set = best_valid_loss\n",
    "\n",
    "        print(f'Best loss for {fs_method}_{dataset_name}: {best_loss_for_HP_set:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
